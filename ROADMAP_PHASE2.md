# Phase 2 Roadmap: "Quarterly Leadership Confidence" Workflow

## Executive Intent
Build a single killer workflow end-to-end: take any incident (manual entry or imported), ensure it is complete and consistent, compute trustworthy metrics, generate leadership-ready outputs (facts, context, narrative), and present a quarterly report with confidence.

This plan assumes **one primary user (you)** and optimizes for: correctness, time saved, confidence, and repeatability. Secondary features stay out of scope unless they directly support quarterly review readiness.

## North Star Outcome (Non-Negotiable)
By the end of Phase 2, you can open the app before a quarterly leadership review and:
1. Select a quarter.
2. See a clear completeness/confidence breakdown for incidents and metrics.
3. Fix gaps quickly (or explicitly accept them with notes).
4. Generate a leadership packet (DOCX/PDF) whose metrics and claims can be traced back to source facts.

## Scope
### In scope
1. Quarterly review workspace, readiness gates, and confidence tooling.
2. Ingestion/integrations that materially reduce manual effort for quarterly reports.
3. Output sharing improvements aligned to real meeting workflows.
4. AI as glue for enrichment and narrative generation, with strict provenance and safe fallbacks.
5. Data model and metric definition hardening to increase trust.
6. Repo hygiene and build/CI guardrails to keep the project reliable.
7. macOS app polish specifically where it improves daily use and leadership presentation.

### Explicit non-goals (Phase 2)
1. Multi-user collaboration and sync across machines.
2. Replacing Jira/ServiceNow/Slack as systems of record.
3. New major feature areas not directly tied to quarterly reporting confidence.
4. Heavy new dependencies unless unavoidable and justified.

## Definitions (to prevent ambiguity)
### "Facts"
Structured, source-attributed, time-stamped data fields used to compute metrics and support reporting.

### "Enrichment"
Derived fields generated by deterministic rules or AI (examples: executive summary, factor categorization, stakeholder update drafts). Enrichments must always record provenance and be safe to ignore.

### "Confidence"
A measurable signal that the quarterly packet is trustworthy. Confidence is not vibes; it is computed from concrete checks (missing timestamps, inconsistent ordering, unknown service, unresolved status, etc.).

## Deliverables (What "Done" Means)
Phase 2 is complete only when the following are true:
1. A dedicated quarterly review surface exists, and it drives report generation.
2. There is a deterministic readiness checklist with pass/fail and clear remediation steps.
3. Reports include a metrics glossary and explicit provenance (what is computed vs entered vs imported vs AI).
4. At least one ingestion path populates incidents, timeline events, and key timestamps sufficiently to reduce manual work.
5. App polish improvements make daily usage feel like a high-quality macOS app (navigation, spacing, typography, chrome, keyboard flow).
6. Repo hygiene is resolved (no hidden local drift), and CI covers the relevant build signals.

## Success Metrics (How We Know This Is Working)
These metrics are designed for a single primary user and leadership-facing confidence.
1. Quarterly packet generation time (goal): <= 5 minutes from opening Quarter Review to exporting DOCX/PDF.
2. Readiness coverage (goal): >= 95% of incidents in the quarter are "Ready" or have an explicit override reason.
3. Trust gap rate (goal): 0 crashes and 0 "unknown/NaN" metric displays in the quarterly packet; any missing data is explicitly labeled.
4. Repeatability (goal): regenerating a packet for the same quarter produces identical metrics when underlying facts have not changed.

## Milestones (Leadership-Level)
1. M1 (end Sprint 2.0): repo hygiene complete, metric definitions documented + tested, CI catches real build failures.
2. M2 (end Sprint 2.1): Quarter Review exists with deterministic readiness gates and remediation UX.
3. M3 (end Sprint 2.2): one ingestion path reduces manual work and improves readiness coverage for a real quarter dataset.
4. M4 (end Sprint 2.3): AI enrichment is integrated with provenance and does not affect metric truth.
5. M5 (optional, end Sprint 2.4): single-button leadership packet and meeting-friendly sharing.

## Execution Cadence (How We Run This)
This is the operating rhythm that keeps Phase 2 "one shot" and predictable.
1. Weekly checkpoint (15-30 minutes):
   - Review readiness coverage trend, packet generation time, and any trust gaps encountered in real use.
2. End-of-sprint demo artifact (required):
   - A generated quarterly packet from a known fixture dataset, plus a short change log of what improved.
3. Engineering gate (required):
   - Canonical checks stay green: `pnpm test:run`, `pnpm test:bundle`, `cd src-tauri && cargo test --lib`, `pnpm tauri build`.

## Dependencies (Explicit)
1. macOS build environment for packaging validation (local and/or CI).
2. At least one real export to target for ingestion (start with Jira CSV; optionally PagerDuty export).
3. Optional: Ollama running locally for AI enrichment validation; Phase 2 must remain fully usable without it.

## Open Decisions (Keep This List Short)
These are the only decisions required to start implementation without ambiguity.
1. Which quarter will be used as the first real-world validation dataset for Phase 2 (recommended: the most recent completed quarter).
2. Jira CSV field mapping:
   - Confirm which columns represent detected/started/acknowledged/resolved times, service identifier, and external reference.
3. Whether PagerDuty export is worth supporting in Phase 2 (only if it materially reduces manual entry for your dataset).

## Timeline and Sequencing (8 Weeks Core + Optional 2-Week Extension)
This is a proposed schedule. The core Phase 2 scope is Sprints 2.0-2.3 (8 weeks). Sprint 2.4 is an optional 2-week extension focused on meeting-friendly sharing and packaging polish.

### Sprint 2.0 (Week 1-2): Guardrails + Repo Hygiene + Metric Contracts
Goal: stabilize the engineering foundation and remove hidden workflow risks before adding product features.

#### 2.0.1 Repo hygiene: eliminate hidden drift risk
Owner: Engineering
Acceptance criteria:
1. No tracked files remain marked `skip-worktree` or `assume-unchanged` unless explicitly documented.
2. `git status` accurately reflects local modifications.
3. A short repo note exists explaining the hygiene policy and how to enforce it.

Implementation steps (explicit):
1. Inventory flags:
   - Command: `git ls-files -t | rg '^[HS] '`
   - Interpretation:
     - `git ls-files -t` prefixes tracked paths with a status tag.
     - `S` means `skip-worktree` (sparse/index-only).
     - `H` means a normal tracked file (not unmerged, not skip-worktree).
2. Identify `skip-worktree`:
   - Command: `git ls-files -t | rg '^S '`
3. Identify `assume-unchanged`:
   - Command: `git ls-files -v | rg '^[a-z] '`
3. Capture and review local diffs before changing flags:
   - Command: `git diff --name-only`
4. Clear flags:
   - Command (safe for paths with spaces):
     - Clear `skip-worktree`:
       - `git ls-files -t -z | perl -0ne 'for (split(/\\0/, $_)) { next unless /^S (.+)$/; print \"$1\\0\"; }' | xargs -0 -I{} git update-index --no-skip-worktree -- \"{}\"`
     - Clear `assume-unchanged`:
       - `git ls-files -v -z | perl -0ne 'for (split(/\\0/, $_)) { next unless /^[a-z] (.+)$/; print \"$1\\0\"; }' | xargs -0 -I{} git update-index --no-assume-unchanged -- \"{}\"`
5. Re-check:
   - Commands:
     - `git ls-files -t | rg '^S '` should return nothing.
     - `git ls-files -v | rg '^[a-z] '` should return nothing.
6. Verify project still passes canonical checks:
   - Commands: `pnpm test:run`, `pnpm test:bundle`, `cd src-tauri && cargo test --lib`, `pnpm tauri build`

Notes:
1. This is non-destructive but may reveal real local changes that were previously hidden.
2. If there are intentional local-only differences, handle them via `.gitignore`, `.git/info/exclude`, or a documented local config file pattern.

#### 2.0.2 Metric contracts: define and lock definitions used in reports
Owner: Product + Engineering
Acceptance criteria:
1. MTTR/MTTA and key dashboard/report metrics have documented definitions (inputs, formula, edge-case behavior).
2. Each metric can be traced to specific fact fields or events.
3. Unit tests exist for edge cases (missing timestamps, timezone, reopened incidents).

Implementation steps:
1. Create a metrics glossary source of truth in Rust (and expose to UI):
   - Data: name, description, formula, required fields, fallback behavior, display formatting.
2. Add tests for metrics:
   - Scope: reopened incidents, partial timestamps, quarter boundary behavior, DST/timezone parsing.
3. Ensure reports embed the glossary as an appendix section.
4. Define quarter inclusion rule (confirmed):
   - Incidents belong to the quarter where `detected_at` falls.
   - Cross-quarter behavior defaults to: show in detected quarter, and surface "Carried Over" if unresolved at quarter end.

#### 2.0.3 Build/CI reliability: ensure we catch "real build" failures
Owner: Engineering
Acceptance criteria:
1. CI includes a macOS job that at minimum verifies the Rust release build path relevant to Tauri packaging.
2. Node version is pinned or documented to reduce env drift.

Implementation steps:
1. Add `.nvmrc` or an equivalent version pin file.
2. Add CI job on `macos-latest` that runs:
   - `pnpm install --frozen-lockfile`
   - `pnpm test:run`
   - `pnpm test:bundle`
   - `pnpm tauri build` (or a reduced, faster, still-meaningful packaging build if runtime is too high)

### Sprint 2.1 (Week 3-4): Quarterly Review Workspace (Core Killer Workflow)
Goal: make quarterly reporting a first-class workflow with explicit readiness checks and a single “Generate Packet” pathway.

#### 2.1.1 Quarterly Review home screen
Owner: Product + Engineering
User story:
1. Pick a quarter.
2. See what’s in scope and what is missing.
3. Click into items that need attention.

Acceptance criteria:
1. Quarter selection and date range are explicit and visible.
2. The view shows:
   - Incident count by priority/severity.
   - MTTR/MTTA summary with confidence status.
   - Top incidents (by impact/downtime) with completeness status.
   - Action item rollup (open, overdue, done).
3. Every “red/yellow” indicator links to a remediation view.

Implementation steps:
1. Define a `QuarterReview` model in backend:
   - Quarter date range.
   - Snapshot timestamp.
   - Frozen metrics (optional at first; required before report export is considered "final").
2. Add a query that returns the quarter rollup in one call (Rust computes, UI renders).
3. Implement the UI route and shell integration (keyboard navigation included).

#### 2.1.2 Readiness checklist (deterministic confidence gates)
Owner: Engineering
Acceptance criteria:
1. Readiness is computed from deterministic checks, not AI.
2. Each failure includes:
   - Exact rule name.
   - Which incidents/fields are failing.
   - Recommended fix steps.
3. A quarter can be "finalized" only if:
   - Either all critical checks pass, or explicit overrides are recorded with notes.
   - Overrides require: rule id, incident ids affected, reason text, timestamp, and "approved_by" (for Phase 2, this is still you, but we record it for auditability).

Checklist rules (minimum set):
1. Timestamp ordering validity:
   - detected_at <= started_at <= acknowledged_at <= resolved_at (with allowed omissions and explicit rules).
2. Required fields present for reportable incidents:
   - title, service, severity, impact, status, detected_at, resolved_at (or explicit “not resolved in quarter” handling).
3. Incident status consistency:
   - No “Resolved” without resolved_at.
4. AI fields do not affect readiness (they are optional).
5. Missing SLA configuration does not crash metrics; it yields explicit "no SLA data" state.
6. Quarter inclusion consistency:
   - Incidents are included by `detected_at` quarter, and unresolved incidents at quarter end are flagged as "Carried Over".

Implementation steps:
1. Implement `ReadinessRule` evaluation in Rust returning structured results.
2. Add tests for each rule.
3. Build a remediation UI:
   - Table of failing items.
   - Jump-to-edit for incidents.
   - Batch actions where safe (example: set missing service using mapping).

#### 2.1.3 Provenance: field-level "where did this come from?"
Owner: Engineering
Acceptance criteria:
1. For key report facts and enrichments, the UI can show provenance:
   - User-entered, imported (source type), computed, AI-generated (model, prompt version).
2. Reports include a provenance appendix or footnote policy.
3. Provenance is queryable and testable:
   - Provenance must be available via a backend query (for report generation) and covered by unit tests for at least one imported field and one AI-enriched field.

Implementation steps:
1. Add metadata columns or a parallel provenance table for tracked fields.
2. When importing, store import source and timestamp.
3. When AI generates, store model id, generation timestamp, and input hash.
4. When deterministic computations run, store the computation version (a constant) and the input facts hash used to compute the output.

### Sprint 2.2 (Week 5-6): Ingestion and Integration (Reduce Manual Work)
Goal: minimize manual entry while preserving trust via mapping, validation, and provenance.

#### 2.2.1 Import pipeline hardening (foundation)
Owner: Engineering
Acceptance criteria:
1. Imports are idempotent or have a clear duplicate strategy.
2. Mapping templates are reusable and versioned.
3. Imports produce readiness-friendly facts (timestamps, service mapping).
4. One source is supported end-to-end as a must-have (Phase 2 default): Jira CSV.

Implementation steps:
1. Normalize import APIs:
   - Shared `ImportJob` structure: source, file(s), mapping template id, preview, execute.
2. Add “preview” mode:
   - Show parsed incidents, missing required fields, and what will be created/updated.
3. Add merge strategies:
   - Match on external_ref or stable key.
4. Add a minimal data dictionary for known sources:
   - Jira CSV.
   - PagerDuty export (CSV/JSON) (optional if it materially helps your real dataset).
   - Generic CSV.

#### 2.2.2 Timeline ingestion (context that leadership asks for)
Owner: Engineering
Acceptance criteria:
1. You can attach a timeline to an incident with timestamps and notes quickly.
2. The timeline can be imported from a file export or paste format with minimal cleanup.
3. Timeline events appear in reports (incident appendix at minimum).

Implementation steps:
1. Define a `timeline_events` table with:
   - incident_id, occurred_at, source, message, optional actor.
2. Add ingestion methods:
   - Paste: accept a simple line format and parse:
     - `YYYY-MM-DD HH:MM <text>`
     - `YYYY-MM-DDTHH:MM:SSZ <text>`
   - File import: accept a structured JSON export format (define a schema) and validate before importing.
3. Add readiness rules:
   - Timeline not required for readiness, but adds a confidence boost indicator.

#### 2.2.3 Service mapping and normalization
Owner: Engineering
Acceptance criteria:
1. Imports map service names to canonical services deterministically.
2. Unmapped services are surfaced in readiness remediation.

Implementation steps:
1. Create a service alias mapping table:
   - alias -> canonical_service_id.
2. Add UI for managing aliases.
3. Ensure imports use alias mapping.

### Sprint 2.3 (Week 7-8): AI as Glue (Enrichment, Not Truth)
Goal: make AI accelerate completeness and narrative without undermining trust.

#### 2.3.1 Enrichment engine with explicit contracts
Owner: Engineering
Acceptance criteria:
1. Enrichments are modeled as jobs with:
   - inputs, outputs, version, provenance, and a deterministic fallback state.
2. AI failures are non-fatal and visible.
3. Users can accept/reject AI suggestions.

Implementation steps:
1. Create `enrichment_jobs` table:
   - job_type, status, input_hash, output_json, model_id, created_at, completed_at, error.
2. Implement job runners:
   - Summaries, stakeholder updates, factor categorization, draft postmortem sections.
3. UI affordances:
   - "Generate" button per enrichment.
   - Diff/preview and accept.
   - Provenance view.

#### 2.3.2 AI-driven interconnect (fill once, propagate everywhere)
Owner: Engineering
Acceptance criteria:
1. When core facts are updated (service, severity, timestamps), all computed metrics update deterministically.
2. AI suggestions appear where they reduce work:
   - Suggested severity/impact based on incident content.
   - Suggested tags and contributing factor categories.
3. No metric in a leadership report depends on AI output.

Implementation steps:
1. Add a clear layering model:
   - Facts -> deterministic computations -> enrichments (AI optional) -> report assembly.
2. Add “recompute” triggers:
   - On incident update, recompute affected summaries and dashboards (without blocking UI).
3. Add tests:
   - Ensure metrics unchanged if AI is disabled.

### Sprint 2.4 (Week 9-10, optional extension): Sharing Outputs the Way Meetings Work
Goal: make it effortless to present and distribute outputs.

#### 2.4.1 Leadership packet generation (single button)
Owner: Product + Engineering
Acceptance criteria:
1. One primary button generates:
   - Quarterly report DOCX.
   - Quarterly report PDF.
   - Optional: incident appendix (separate file) if large.
2. The packet includes:
   - Metrics glossary.
   - Confidence checklist summary and overrides (if any).
   - Provenance policy.
   - "Assumptions and known gaps" section if any readiness overrides exist.
3. The packet has a stable, leadership-oriented structure (table of contents):
   - Executive summary.
   - Metrics overview (MTTR/MTTA, volume, distribution).
   - Notable incidents (top 5 by impact/downtime) with context and timeline.
   - Trends and discussion points (deterministic rules + optional AI suggestions clearly labeled).
   - Action items summary (opened/closed/overdue).
   - Appendices: metrics glossary, provenance policy, carried-over incidents, overrides.

Implementation steps:
1. Report assembly refactor to be quarter-centric.
2. Add report metadata and history:
   - Quarter, generated_at, file format(s), version, and inputs hash.
3. Freeze-on-finalize behavior:
   - When the quarter is finalized, generate and store a snapshot of the rollups and report inputs hash so regenerations are consistent unless facts change.

#### 2.4.2 Sharing UX
Owner: Engineering
Acceptance criteria:
1. "Reveal in Finder" works reliably.
2. "Copy summary to clipboard" exists for stakeholder updates and meeting notes.
3. Exports include Markdown/CSV as a bridge format (optional but useful).

Implementation steps:
1. Use Tauri plugins to open/reveal paths.
2. Provide a standardized export directory option.

### Phase 2 Polish Track (runs alongside Sprints 2.1-2.4)
Goal: “looks like a Mac app from the future” where it matters for daily use and presentation.

#### 2.P1 Visual direction and layout system
Owner: Product + Engineering
Acceptance criteria:
1. Define a single visual direction for the whole app (typography, spacing scale, chrome).
2. Establish design tokens used by Tailwind:
   - type scale, radii, elevations, surface colors, accent, separators.

Implementation steps:
1. Add a `design-tokens` module (CSS variables and Tailwind config alignment).
2. Ensure consistent spacing and typography in:
   - Sidebar, Quarter Review, Incident Detail, Report generation screens.

#### 2.P2 macOS chrome and ergonomics
Owner: Engineering
Acceptance criteria:
1. App has polished window behavior (titlebar, menus, keyboard shortcuts).
2. Navigation is fast without mouse (command palette, shortcuts, focus states).

Implementation steps:
1. Audit and standardize keyboard shortcuts.
2. Add macOS-appropriate window chrome configuration in Tauri.

## Data Improvements (Quality and Trust)
These run in support of the killer workflow and confidence.

### Data quality rules (must-have)
1. All timestamps stored in a consistent format with timezone handling.
2. Quarter inclusion rule (decided):
   - An incident belongs to the quarter where `detected_at` falls.
   - Example: detected Mar 31, resolved Apr 1 => Q1.
   - Cross-quarter handling (Phase 2 default): include the incident only in its `detected_at` quarter, and list it in a "Spans Quarters / Carried Over" section if unresolved by quarter end.
3. Consistent reopen behavior:
   - A reopened incident must not corrupt MTTR; define handling and test it.

### Traceability (must-have)
1. Every metric in the report references:
   - the underlying fields/events used.
2. Imports and AI enrichments record provenance.

## Verification Strategy (Required for Every Section)
For each major deliverable above:
1. Unit tests for rule evaluation and metrics.
2. Integration-style tests for:
   - import preview and execution.
   - report generation for a small fixture dataset.
3. Canonical commands must stay green:
   - `pnpm test:run`
   - `pnpm test:bundle`
   - `cd src-tauri && cargo test --lib`
   - `pnpm tauri build`

## Risks and Mitigations
1. Risk: AI becomes a crutch and undermines trust.
   - Mitigation: metrics never depend on AI; provenance always visible; accept/reject workflow.
2. Risk: scope creep returns.
   - Mitigation: quarter workflow gates all work; new features must tie directly to readiness or report quality.
3. Risk: import complexity explodes.
   - Mitigation: start with one strong import path and a generalized pipeline, not 5 half-integrations.
4. Risk: performance degrades with real data.
   - Mitigation: keep computations in Rust, batch queries, and add dataset-size regression tests.

## Immediate Next Actions (First 5 Working Days)
1. Execute Sprint 2.0.1 repo hygiene and document the policy.
2. Add Node version pinning and align local/CI toolchain expectations.
3. Draft metric glossary entries for MTTR/MTTA and quarter inclusion rules (confirmed: `detected_at`).
4. Create the Quarter Review route skeleton and define the Rust response shape.
5. Add the first readiness rules (timestamp ordering + missing required fields) and their tests.
